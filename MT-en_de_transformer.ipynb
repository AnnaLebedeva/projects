{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Yj7aripVIsbG",
      "metadata": {
        "id": "Yj7aripVIsbG"
      },
      "source": [
        "Проект представляет собой реализацию задачи обучения трансформера на языковой паре en-de и оценки качества получившейся модели.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-akGUuAS1pyk",
        "outputId": "0f9cbb86-56d0-4e53-fe38-b2dfe96a80fe"
      },
      "id": "-akGUuAS1pyk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 7.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05d202c4",
      "metadata": {
        "id": "05d202c4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers import normalizers\n",
        "from tokenizers.normalizers import Lowercase\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "from tokenizers import decoders\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем данные:"
      ],
      "metadata": {
        "id": "dT6mxVR_SRcQ"
      },
      "id": "dT6mxVR_SRcQ"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/de-en/opus.de-en-train.de\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/de-en/opus.de-en-train.en\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/de-en/opus.de-en-test.de\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/de-en/opus.de-en-test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPM7XE5rCf4x",
        "outputId": "5f9225a7-6f15-4b21-dec1-57d475913651"
      },
      "id": "gPM7XE5rCf4x",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-22 21:13:40--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/de-en/opus.de-en-train.de\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75998572 (72M)\n",
            "Saving to: ‘opus.de-en-train.de.1’\n",
            "\n",
            "opus.de-en-train.de 100%[===================>]  72.48M  20.7MB/s    in 4.4s    \n",
            "\n",
            "2022-05-22 21:13:45 (16.6 MB/s) - ‘opus.de-en-train.de.1’ saved [75998572/75998572]\n",
            "\n",
            "--2022-05-22 21:13:45--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/de-en/opus.de-en-train.en\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 70247384 (67M)\n",
            "Saving to: ‘opus.de-en-train.en.1’\n",
            "\n",
            "opus.de-en-train.en 100%[===================>]  66.99M  20.1MB/s    in 4.1s    \n",
            "\n",
            "2022-05-22 21:13:50 (16.2 MB/s) - ‘opus.de-en-train.en.1’ saved [70247384/70247384]\n",
            "\n",
            "--2022-05-22 21:13:50--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/de-en/opus.de-en-test.de\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 165307 (161K)\n",
            "Saving to: ‘opus.de-en-test.de.1’\n",
            "\n",
            "opus.de-en-test.de. 100%[===================>] 161.43K   389KB/s    in 0.4s    \n",
            "\n",
            "2022-05-22 21:13:51 (389 KB/s) - ‘opus.de-en-test.de.1’ saved [165307/165307]\n",
            "\n",
            "--2022-05-22 21:13:51--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/de-en/opus.de-en-test.en\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 153644 (150K)\n",
            "Saving to: ‘opus.de-en-test.en.1’\n",
            "\n",
            "opus.de-en-test.en. 100%[===================>] 150.04K   333KB/s    in 0.5s    \n",
            "\n",
            "2022-05-22 21:13:52 (333 KB/s) - ‘opus.de-en-test.en.1’ saved [153644/153644]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_sents = open('opus.de-en-train.en').read().lower().splitlines()\n",
        "de_sents = open('opus.de-en-train.de').read().lower().splitlines()"
      ],
      "metadata": {
        "id": "25m-WA9MCf7A"
      },
      "id": "25m-WA9MCf7A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Воспользуемся токенизатором."
      ],
      "metadata": {
        "id": "capKUIajMIKh"
      },
      "id": "capKUIajMIKh"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = Tokenizer(WordPiece(), ) # WordPiece разбивает текст не на слова, а на символьные н-граммы\n",
        "\n",
        "tokenizer_en.normalizer = normalizers.Sequence([Lowercase()]) # приводим к нижнему регистру\n",
        "# записываем normalizers.Sequence([Lowercase()]) в атрибут представителя класса Tokenizer\n",
        "tokenizer_en.pre_tokenizer = Whitespace() # разбиваем по пробелам\n",
        "# записываем Whitespace() в атрибут представителя класса Tokenizer\n",
        "\n",
        "trainer_en = WordPieceTrainer(\n",
        "          vocab_size=30000, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\"])\n",
        "tokenizer_en.train(files=[\"opus.de-en-train.en\"], trainer=trainer_en )\n",
        "\n",
        "tokenizer_de = Tokenizer(WordPiece(), )\n",
        "tokenizer_de.normalizer = normalizers.Sequence([Lowercase()])\n",
        "tokenizer_de.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer_de = WordPieceTrainer(\n",
        "          vocab_size=30000, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\"])\n",
        "tokenizer_de.train(files=[\"opus.de-en-train.de\"], trainer=trainer_de )\n",
        "# тут словари для двух языков одинаковые, но они могут быть и разные. Может быть разный размер."
      ],
      "metadata": {
        "id": "jvONIb5iCf9P"
      },
      "id": "jvONIb5iCf9P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# раскоментируйте эту ячейку при обучении токенизатора\n",
        "# а потом снова закоментируйте чтобы при перезапуске не перезаписать токенизаторы\n",
        "# tokenizer_en.save('tokenizer_en')\n",
        "# tokenizer_de.save('tokenizer_de')"
      ],
      "metadata": {
        "id": "R_yA92hxCf_w"
      },
      "id": "R_yA92hxCf_w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")\n",
        "tokenizer_de = Tokenizer.from_file(\"tokenizer_de\")"
      ],
      "metadata": {
        "id": "wh1dTbWGCgCW"
      },
      "id": "wh1dTbWGCgCW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text, tokenizer, target=False):\n",
        "    return [tokenizer.token_to_id('[CLS]')] + tokenizer.encode(text).ids + [tokenizer.token_to_id('[SEP]')]\n",
        "# Переводим текст в индексы вот таким образом. В начало добавляем токен '[CLS]', а в конец '[SEP]'."
      ],
      "metadata": {
        "id": "rzYfGE1YCgEq"
      },
      "id": "rzYfGE1YCgEq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Кодируем и паддим\n",
        "X_en = [encode(t, tokenizer_en) for t in en_sents]\n",
        "X_de = [encode(t, tokenizer_de, target=True) for t in de_sents]"
      ],
      "metadata": {
        "id": "Qkj-_AAPCgHG"
      },
      "id": "Qkj-_AAPCgHG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_en = np.mean([len(x) for x in X_en])\n",
        "max_len_de = np.mean([len(x) for x in X_de])\n",
        "max_len_en, max_len_de"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d7LpEcOCgJk",
        "outputId": "667c4c41-6953-4087-c5aa-7fdde393ab2f"
      },
      "id": "0d7LpEcOCgJk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18.466743, 18.535871)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ограничимся длинной в 20 и 22 (разные чтобы показать что в seq2seq не нужна одинаковая длина)\n",
        "max_len_en, max_len_de = 20, 22"
      ],
      "metadata": {
        "id": "IJhcfhPICgMC"
      },
      "id": "IJhcfhPICgMC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# важно следить чтобы индекс паддинга совпадал в токенизаторе с value в pad_sequences\n",
        "PAD_IDX = tokenizer_de.token_to_id('[PAD]')\n",
        "PAD_IDX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cwTunuQCgOf",
        "outputId": "3ade098d-b7dd-406f-d455-88779c64a73c"
      },
      "id": "7cwTunuQCgOf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en.token_to_id('[PAD]')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qDZoGAfCgRM",
        "outputId": "a310d324-1cd8-414b-bc08-4ed1611a3602"
      },
      "id": "3qDZoGAfCgRM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_en = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "              X_en, maxlen=max_len_en, padding='post', value=tokenizer_en.token_to_id('[PAD]'))\n",
        "# здесь нам важно подать, что мы падим не нулем, а индексом тега PAD в токенизаторе\n",
        "# англ текст подаем целиком и ничего специально не надо делать, тк в этом корпусе все тексты коннектятся к английскому\n",
        "X_de_out = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "              [x[1:] for x in X_de], maxlen=max_len_de-1, padding='post', \n",
        "              value=tokenizer_en.token_to_id('[PAD]'))\n",
        "# второй язык нам надо подать в трансформер 2 раза, один раз как таргет, один раз как инпут для декодера.\n",
        "# здесь нужно подать все сдвинутое на 1 индекс вправо\n",
        "X_de_dec = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "              [x[:-1] for x in X_de], maxlen=max_len_de-1, \n",
        "              padding='post', value=tokenizer_de.token_to_id('[PAD]'))\n",
        "# а здесь на один индекс влево"
      ],
      "metadata": {
        "id": "dVZzD_keNxgb"
      },
      "id": "dVZzD_keNxgb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# миллион примеров \n",
        "X_en.shape, X_de_out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqFaqIA7Nxpm",
        "outputId": "ae8cd843-6bbc-4609-a09f-67f84418c14b"
      },
      "id": "hqFaqIA7Nxpm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1000000, 20), (1000000, 21))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Разделяем на трейн и тест\n",
        "X_en_train, X_en_valid, X_de_dec_train, X_de_dec_valid, X_de_out_train, X_de_out_valid = train_test_split(X_en, \n",
        "                                                                                                      X_de_dec, \n",
        "                                                                                                      X_de_out, \n",
        "                                                                                                      test_size=0.05)"
      ],
      "metadata": {
        "id": "5Q2fG4kzNxsG"
      },
      "id": "5Q2fG4kzNxsG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перейдём к модели."
      ],
      "metadata": {
        "id": "LqCLoxS1Wqdj"
      },
      "id": "LqCLoxS1Wqdj"
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "    \"\"\"Calculate the attention weights. \"\"\"\n",
        "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "    logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "    if mask is not None:\n",
        "        logits += (mask * -1e9)\n",
        "\n",
        "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, value)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "fHBH1a2gNxu5"
      },
      "id": "fHBH1a2gNxu5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "        super(MultiHeadAttention, self).__init__(name=name)\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    def split_heads(self, inputs, batch_size):\n",
        "        inputs = tf.reshape(\n",
        "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "            'value'], inputs['mask']\n",
        "        batch_size = tf.shape(query)[0]\n",
        "\n",
        "        query = self.query_dense(query)\n",
        "        key = self.key_dense(key)\n",
        "        value = self.value_dense(value)\n",
        "\n",
        "        query = self.split_heads(query, batch_size)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))\n",
        "\n",
        "        outputs = self.dense(concat_attention)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "dzV6yP3-Nx0K"
      },
      "id": "dzV6yP3-Nx0K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(x):\n",
        "    mask = tf.cast(tf.math.equal(x, PAD_IDX), tf.float32)\n",
        "\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]"
      ],
      "metadata": {
        "id": "90kuD65ONx3m"
      },
      "id": "90kuD65ONx3m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_look_ahead_mask(x):\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    padding_mask = create_padding_mask(x)\n",
        "    return tf.maximum(look_ahead_mask, padding_mask)"
      ],
      "metadata": {
        "id": "C6sDo1nANx8g"
      },
      "id": "C6sDo1nANx8g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, position, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model=d_model)\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ],
      "metadata": {
        "id": "0xc0iwGDNx_s"
      },
      "id": "0xc0iwGDNx_s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "    attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "XmMV1jBzNyH9"
      },
      "id": "XmMV1jBzNyH9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            max_len,\n",
        "            name=\"encoder\"):\n",
        "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
        "\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = encoder_layer(\n",
        "            units=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            name=\"encoder_layer_{}\".format(i),\n",
        "        )([outputs, padding_mask])\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "o-9M_aEXCgTn"
      },
      "id": "o-9M_aEXCgTn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "    look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    attention1 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': look_ahead_mask\n",
        "      })\n",
        "    attention1 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "    attention2 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
        "          'query': attention1,\n",
        "          'key': enc_outputs,\n",
        "          'value': enc_outputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "    attention2 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ],
      "metadata": {
        "id": "9jA_Cg5eCgWH"
      },
      "id": "9jA_Cg5eCgWH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            max_len,\n",
        "            name='decoder'):\n",
        "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "    look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name='look_ahead_mask')\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
        "\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = decoder_layer(\n",
        "            units=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            name='decoder_layer_{}'.format(i),\n",
        "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "    return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ],
      "metadata": {
        "id": "46Hs6oFXXQny"
      },
      "id": "46Hs6oFXXQny",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer(vocab_size,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                max_len,\n",
        "                name=\"transformer\"):\n",
        "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "    enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(inputs)\n",
        "\n",
        "    look_ahead_mask = tf.keras.layers.Lambda(\n",
        "      create_look_ahead_mask,\n",
        "      output_shape=(1, None, None),\n",
        "      name='look_ahead_mask')(dec_inputs)\n",
        "\n",
        "    dec_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='dec_padding_mask')(inputs)\n",
        "\n",
        "    enc_outputs = encoder(\n",
        "      vocab_size=vocab_size[0],\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "      max_len=max_len[0],\n",
        "    )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "    dec_outputs = decoder(\n",
        "      vocab_size=vocab_size[1],\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "      max_len=max_len[1],\n",
        "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=vocab_size[1], name=\"outputs\")(dec_outputs)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "6qn1NMLQXQqm"
      },
      "id": "6qn1NMLQXQqm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L  = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none',)\n",
        "# поскольку у нас задача классификации, для нее стандартный лосс это CategoricalCrossentropy\n",
        "# а тк мы передаем векторы индексов, то мы используем SparseCategoricalCrossentropy\n",
        "def loss_function(y_true, y_pred):\n",
        "    loss = L(y_true, y_pred)\n",
        "\n",
        "    mask = tf.cast(tf.not_equal(y_true, PAD_IDX), tf.float32)\n",
        "    loss = tf.multiply(loss, mask)\n",
        "\n",
        "    return tf.reduce_mean(loss)"
      ],
      "metadata": {
        "id": "0xAj-xPUXQtL"
      },
      "id": "0xAj-xPUXQtL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "# через warmup_steps мы определяем, до какого момента будем повышать lr (\"разогрев\"), чтобы потом понижать\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "Gigo-BN1XQwI"
      },
      "id": "Gigo-BN1XQwI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Определяем параметры модели.\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# small model\n",
        "NUM_LAYERS = 2 # кол-во слоев\n",
        "D_MODEL = 256 # размерность dense слоев внутри трансформера, т.е. сколько голов, т.е. у нас каждый блок трансформера, где мы считаем attention,\n",
        "# и переходим от эмбедингов в новое представление, которое с учетом аттеншена рассчитано, вот это один блок (одна голова)\n",
        "NUM_HEADS = 8 # у нас 8 таких блоков\n",
        "UNITS = 512\n",
        "DROPOUT = 0.1\n",
        "\n",
        "\n",
        "# average model\n",
        "# NUM_LAYERS = 6\n",
        "# D_MODEL = 512\n",
        "# NUM_HEADS = 8\n",
        "# UNITS = 2048\n",
        "# DROPOUT = 0.1\n",
        "\n",
        "# инициализируем модель\n",
        "# первые 2 строчки отвечают за то если есть 2 видеокарты\n",
        "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
        "with mirrored_strategy.scope():\n",
        "    model = transformer(\n",
        "        vocab_size=(tokenizer_en.get_vocab_size(),tokenizer_de.get_vocab_size()),\n",
        "        num_layers=NUM_LAYERS,\n",
        "        units=UNITS,\n",
        "        d_model=D_MODEL,\n",
        "        num_heads=NUM_HEADS,\n",
        "        dropout=DROPOUT,\n",
        "        max_len=[max_len_en, max_len_de])\n",
        "\n",
        "#     learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        0.001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "    def accuracy(y_true, y_pred):\n",
        "#         y_true = tf.reshape(y_true, shape=(-1, max_len_ru - 1))\n",
        "        return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint('model_ende',\n",
        "                                                monitor='val_loss',\n",
        "                                                verbose=1,\n",
        "                                            save_weights_only=True,\n",
        "                                            save_best_only=True,\n",
        "                                            mode='min',\n",
        "                                            save_freq='epoch')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6ITgQSbXQzA",
        "outputId": "79953517-101f-48b3-c3bb-7c2b7d98c40e"
      },
      "id": "O6ITgQSbXQzA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit((X_en_train, X_de_dec_train), X_de_out_train, \n",
        "             validation_data=((X_en_valid, X_de_dec_valid), X_de_out_valid),\n",
        "             batch_size=200,\n",
        "             epochs=100,\n",
        "             callbacks=[checkpoint]\n",
        "             )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "k2x8PvkxXQ1z",
        "outputId": "3d872126-4579-4d85-f831-f35be7db7ab1"
      },
      "id": "k2x8PvkxXQ1z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "4750/4750 [==============================] - ETA: 0s - loss: 2.5210 - accuracy: 0.2370\n",
            "Epoch 1: val_loss improved from inf to 1.94233, saving model to model_ruen\n",
            "4750/4750 [==============================] - 850s 177ms/step - loss: 2.5210 - accuracy: 0.2370 - val_loss: 1.9423 - val_accuracy: 0.2980\n",
            "Epoch 2/100\n",
            " 140/4750 [..............................] - ETA: 13:15 - loss: 1.9133 - accuracy: 0.2961"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-a9d30595e27f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m              \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m              \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m              )\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# не имеет смысла сравнивать, за сколько эпох обучилась модель, есть смысл сравнивать, за сколько шагов, то есть изменений весов\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text):\n",
        "    input_ids = encode(text.lower(), tokenizer_en)\n",
        "\n",
        "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "                                      [input_ids], maxlen=max_len_en, padding='post',\n",
        "                                      value = 3)\n",
        "    \n",
        "    \n",
        "    output_ids = [tokenizer_de.token_to_id('[CLS]') ]\n",
        "    pred = model((input_ids,                   tf.cast([output_ids], tf.int32)), training=False)\n",
        "\n",
        "# генерируем до тега SEP\n",
        "    while pred.numpy().argmax(2)[0][-1] not in [tokenizer_de.token_to_id('[SEP]'),\n",
        "                                                            ]:\n",
        "\n",
        "        if len(output_ids) > max_len_de:\n",
        "            break\n",
        "        output_ids.append(pred.numpy().argmax(2)[0][-1])\n",
        "        pred = model((input_ids, tf.cast([output_ids], tf.int32)), training=False)\n",
        "\n",
        "    return tokenizer_de.decode(output_ids[1:])"
      ],
      "metadata": {
        "id": "zPutEgTnXQ4q"
      },
      "id": "zPutEgTnXQ4q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посчитаем BLEU."
      ],
      "metadata": {
        "id": "qYx8aze1xvNl"
      },
      "id": "qYx8aze1xvNl"
    },
    {
      "cell_type": "code",
      "source": [
        "len(en_sents_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qdaC-LI2iqT",
        "outputId": "95d49fa6-847b-416c-b976-ccb683a1db26"
      },
      "id": "_qdaC-LI2iqT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "en_sents_test = open('opus.de-en-test.en').read().lower().splitlines()\n",
        "de_sents_test = open('opus.de-en-test.de').read().lower().splitlines()\n",
        "\n",
        "translations = []\n",
        "\n",
        "for i in range(len(en_sents_test)):\n",
        "  if i % 100 == 0:\n",
        "    print(i)\n",
        "  translations.append(translate(en_sents_test[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kmh88gIlLaw",
        "outputId": "75736205-bc51-4585-bd85-1ffd6e4ebebd"
      },
      "id": "_kmh88gIlLaw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_sents_test[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0oZHhQspD9x",
        "outputId": "56ebe011-d43a-4f2d-c338-46cf7e1c68d9"
      },
      "id": "m0oZHhQspD9x",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['04:26:35',\n",
              " 'prähistorische archäologie im dritten reich\".',\n",
              " \"by clicking on 'save profile', you the user agree to these terms and conditions.\",\n",
              " 'i wanted to show you something first.',\n",
              " 'you have suffered because of shinkichi.',\n",
              " 'moodle:bg-bab: calendar: day view: friday, 25 august 1989',\n",
              " \"i mean, most people, they see another person walking down the street with that big heavy bag they're carrying, and they just walk on by.\",\n",
              " '(iii) the degree of substitution of different factors in pro duction.',\n",
              " 'content: 32 s., 17 abb., 14 taf.',\n",
              " \"so i have to withdraw as bobby's attorney.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "de_sents_test[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xjj3KfqCpSyi",
        "outputId": "d2253e7c-8f56-40a1-b086-17331f33c7d6"
      },
      "id": "Xjj3KfqCpSyi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['04:26:35',\n",
              " 'prähistorische archäologie im dritten reich\".',\n",
              " 'die nutzungsbedingungen werden durch das klicken des nutzers auf \"profil speichern\" vereinbart.',\n",
              " 'ich wollte dir erst noch etwas zeigen.',\n",
              " 'du musstest wegen shinkichi leiden.',\n",
              " 'moodle:bg-bab: kalender: tagesansicht: freitag, 25. august 1989',\n",
              " 'ich meine, die meisten leuten sehen eine andere person die straße entlang laufen und diesen großen, schweren koffer tragen und sie laufen einfach vorbei.',\n",
              " \"regressivitätsverhältnis ( ' )\",\n",
              " 'inhalt: 88 s., 14 abb., 6 taf., 1 beil.',\n",
              " 'aus diesem grund muss ich das mandat für bobby niederlegen.']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translations[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdNDfYDnpBFq",
        "outputId": "e5d175dc-856c-4892-e1dd-07be01b3d95d"
      },
      "id": "rdNDfYDnpBFq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['04 : 26 : 35',\n",
              " 'pru ##to ##mu ##ti ##on arch ##ik ##t ##ten im empir ##e ##um ##um .',\n",
              " \"durch klicken sie auf ' speichern , können sie die benutzer zu diesen bedingungen und bedingungen zustimmen .\",\n",
              " 'ich wollte euch etwas zeigen .',\n",
              " 'sie haben wegen shin ##ki ##chi .',\n",
              " 'moodle : bg - bab : kalender : tagesansicht : freitag , 25 . august 1989',\n",
              " 'die straße mit der schweren tü ##te , und sie können nur durch ##ziehen .',\n",
              " 'iii ) die grad der un ##passen ##de faktoren in pro you - erfahrung .',\n",
              " 'inhalt : 32 s ., 17 abb ., 14 taf .',\n",
              " 'also muss ich das als bobby ##s anwalt zurück ##nehmen .']"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleus = []\n",
        "\n",
        "for i, t in enumerate(translations):\n",
        "  reference = tokenizer_de.encode(t).tokens\n",
        "  hypothesis = tokenizer_de.encode(de_sents_test[i]).tokens\n",
        "\n",
        "bleus.append(nltk.translate.bleu_score.sentence_bleu([reference], hypothesis,  ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvUrekRocEf2",
        "outputId": "e0e66b52-a771-4ec5-8956-07adfa4b7ef4"
      },
      "id": "SvUrekRocEf2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(sum(bleus)/len(bleus))*100"
      ],
      "metadata": {
        "id": "_zc2LNS7058M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75f3d5a0-ec28-44d8-902f-00308e17c021"
      },
      "id": "_zc2LNS7058M",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58.202727065518864"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}